# LiteAI-SDK Usage Guide for LLMs

## Overview

**LiteAI-SDK** is a Python wrapper around LiteLLM that provides an intuitive, [AI SDK](https://ai-sdk.dev/)-inspired developer experience for building LLM-powered applications. It simplifies common patterns like text generation, streaming, tool calling, and agentic workflows.

**Key Features:**
- Unified API for multiple LLM providers (via LiteLLM)
- Synchronous and asynchronous support
- Built-in streaming capabilities
- Automatic tool execution with type inference
- Strong type hints and Pydantic integration
- Agent loop pattern support

## Installation

```bash
pip install liteai-sdk
```

## Core Concepts

### 1. LLM Client Initialization

Create an LLM client by specifying a provider, API key, and base URL:

```python
from liteai_sdk import LLM, LlmProviders

llm = LLM(
    provider=LlmProviders.OPENAI,  # Provider enum
    api_key="your-api-key",
    base_url="https://api.openai.com/v1"  # Optional custom endpoint
)
```

**Supported Providers:** Any provider supported by LiteLLM (OpenAI, Anthropic, Azure, etc.)

### 2. Message Types

The SDK provides strongly-typed message classes:

- **`UserMessage(content)`** - User input message
- **`SystemMessage(content)`** - System prompt/instructions
- **`AssistantMessage(content, reasoning_content, tool_calls)`** - LLM response
- **`ToolMessage(tool_call_id, content)`** - Tool execution result

**Example:**

```python
from liteai_sdk import UserMessage, SystemMessage

messages = [
    SystemMessage("You are a helpful assistant."),
    UserMessage("Hello, world!")
]
```

### 3. Request Parameters

Configure requests using `LlmRequestParams`:

```python
from liteai_sdk import LlmRequestParams

params = LlmRequestParams(
    model="gpt-4",
    messages=[UserMessage("Hello")],
    tools=None,                    # Optional: list of tools
    tool_choice="auto",           # "auto" | "required" | "none"
    execute_tools=False,          # Auto-execute tool calls
    timeout_sec=30.0,             # Request timeout
    temperature=0.7,              # Sampling temperature
    max_tokens=1000,              # Max response tokens
    headers={"X-Custom": "value"}, # Extra headers
    extra_args={}                 # Additional provider-specific args
)
```

## Basic Usage Patterns

### Pattern 1: Simple Text Generation (Synchronous)

Generate a single response synchronously:

```python
from liteai_sdk import LLM, LlmProviders, LlmRequestParams, UserMessage

llm = LLM(provider=LlmProviders.OPENAI, api_key="...", base_url="...")

response = llm.generate_text_sync(
    LlmRequestParams(
        model="gpt-4",
        messages=[UserMessage("What is Python?")]
    )
)

# response is a list: [AssistantMessage]
print(response[0].content)
```

### Pattern 2: Simple Text Generation (Asynchronous)

For async contexts:

```python
import asyncio

async def main():
    llm = LLM(provider=LlmProviders.OPENAI, api_key="...", base_url="...")
    
    response = await llm.generate_text(
        LlmRequestParams(
            model="gpt-4",
            messages=[UserMessage("What is Python?")]
        )
    )
    
    print(response[0].content)

asyncio.run(main())
```

### Pattern 3: Streaming Text (Synchronous)

Stream tokens as they're generated:

```python
stream, full_message_queue = llm.stream_text_sync(
    LlmRequestParams(
        model="gpt-4",
        messages=[UserMessage("Tell me a story")]
    )
)

# Stream chunks in real-time
for chunk in stream:
    if chunk.content:
        print(chunk.content, end="", flush=True)

print("\n")

# Access complete message after streaming
while (message := full_message_queue.get()) is not None:
    print(f"Complete: {message}")
```

**Returns:** 
- `stream`: Generator yielding `AssistantMessageChunk` objects
- `full_message_queue`: Queue containing complete `AssistantMessage` (or `None` when done)

### Pattern 4: Streaming Text (Asynchronous)

```python
async def main():
    stream, full_message_queue = await llm.stream_text(
        LlmRequestParams(
            model="gpt-4",
            messages=[UserMessage("Tell me a story")]
        )
    )
    
    async for chunk in stream:
        if chunk.content:
            print(chunk.content, end="", flush=True)
    
    print("\n")
    
    while (message := await full_message_queue.get()) is not None:
        print(f"Complete: {message}")

asyncio.run(main())
```

## Tool Calling

### Tool Definition Options

The SDK supports three ways to define tools:

#### Option 1: Python Function (Recommended)

Use docstrings and type hints for automatic schema generation:

```python
def get_weather(location: str, unit: str = "celsius") -> str:
    """
    Get the current weather in a given location.
    
    Args:
        location: The city and state, e.g. San Francisco, CA
        unit: Temperature unit (celsius or fahrenheit)
    
    Returns:
        Weather information as a string
    """
    return f"Weather in {location}: 72°{unit[0].upper()}"

# Docstring becomes tool description
# Type hints become parameter schemas
# Default values make parameters optional
```

#### Option 2: ToolDef Class

For more control over naming and description:

```python
from liteai_sdk import ToolDef

def weather_impl(location: str, unit: str = "celsius") -> str:
    return f"Weather in {location}: 72°{unit[0].upper()}"

tool = ToolDef(
    name="get_weather",
    description="Get current weather for any location",
    execute=weather_impl
)
```

#### Option 3: Raw Dictionary (OpenAI Format)

For maximum control or provider-specific tools:

```python
raw_tool = {
    "name": "get_weather",
    "description": "Get the current weather in a given location",
    "parameters": {
        "type": "object",
        "properties": {
            "location": {
                "type": "string",
                "description": "The city and state, e.g. San Francisco, CA"
            },
            "unit": {
                "type": "string",
                "enum": ["celsius", "fahrenheit"]
            }
        },
        "required": ["location"]
    }
}
```

### Pattern 5: Tool Calling with Manual Handling

Let the LLM decide when to call tools, but handle execution yourself:

```python
def search_web(query: str) -> str:
    """Search the web for information."""
    return f"Search results for: {query}"

response = llm.generate_text_sync(
    LlmRequestParams(
        model="gpt-4",
        messages=[UserMessage("Search for Python tutorials")],
        tools=[search_web],
        execute_tools=False  # Don't auto-execute
    )
)

assistant_msg = response[0]
if assistant_msg.tool_calls:
    print(f"LLM wants to call: {assistant_msg.tool_calls}")
    # Handle tool execution manually
```

### Pattern 6: Automatic Tool Execution

Enable `execute_tools=True` to automatically run tool functions:

```python
def calculate(expression: str) -> float:
    """
    Evaluate a mathematical expression.
    
    Args:
        expression: Math expression like "2 + 2"
    """
    return eval(expression)

response = llm.generate_text_sync(
    LlmRequestParams(
        model="gpt-4",
        messages=[UserMessage("What is 25 * 4?")],
        tools=[calculate],
        execute_tools=True  # Auto-execute tools
    )
)

# response contains: [AssistantMessage, ToolMessage, ...]
for msg in response:
    if msg.role == "assistant":
        print(f"Assistant: {msg.content}")
    elif msg.role == "tool":
        print(f"Tool result: {msg.content}")
```

### Pattern 7: Async Tool Execution

Tools can be async functions:

```python
import asyncio

async def fetch_data(url: str) -> str:
    """
    Fetch data from a URL asynchronously.
    
    Args:
        url: The URL to fetch
    """
    await asyncio.sleep(0.1)  # Simulate async I/O
    return f"Data from {url}"

async def main():
    response = await llm.generate_text(
        LlmRequestParams(
            model="gpt-4",
            messages=[UserMessage("Fetch https://example.com")],
            tools=[fetch_data],
            execute_tools=True
        )
    )
    
    for msg in response:
        print(f"{msg.role}: {msg.content}")

asyncio.run(main())
```

### Pattern 8: Streaming with Tool Calls

Combine streaming with automatic tool execution:

```python
def get_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%H:%M:%S")

stream, full_message_queue = llm.stream_text_sync(
    LlmRequestParams(
        model="gpt-4",
        messages=[UserMessage("What time is it?")],
        tools=[get_time],
        execute_tools=True
    )
)

# Stream assistant response
for chunk in stream:
    if chunk.content:
        print(chunk.content, end="", flush=True)

print("\n")

# Get complete messages including tool results
while (msg := full_message_queue.get()) is not None:
    print(f"{msg.role}: {msg.content if hasattr(msg, 'content') else msg}")
```

## Advanced Patterns

### Pattern 9: Agent Loop

Build autonomous agents that use tools iteratively:

```python
from liteai_sdk import SystemMessage, AssistantMessage, ToolMessage

SYSTEM_PROMPT = """You are a helpful assistant with access to tools.
Call attempt_completion when you've finished the task."""

def read_file(path: str) -> str:
    """Read a file from disk."""
    with open(path, 'r') as f:
        return f.read()

def agent_loop():
    is_running = True
    
    def attempt_completion() -> str:
        """Call this when the task is complete."""
        nonlocal is_running
        is_running = False
        return "Task completed"
    
    llm = LLM(provider=LlmProviders.OPENAI, api_key="...", base_url="...")
    messages = [
        SystemMessage(SYSTEM_PROMPT),
        UserMessage("Read example.txt and summarize it")
    ]
    tools = [read_file, attempt_completion]
    
    while is_running:
        response = llm.generate_text_sync(
            LlmRequestParams(
                model="gpt-4",
                messages=messages,
                tools=tools,
                execute_tools=True
            )
        )
        
        # Add responses to conversation history
        messages.extend(response)
        
        # Process responses
        for msg in response:
            if isinstance(msg, AssistantMessage):
                print(f"Assistant: {msg.content}")
            elif isinstance(msg, ToolMessage):
                print(f"Tool executed: {msg.content[:100]}...")

agent_loop()
```

### Pattern 10: Multi-Turn Conversation

Maintain context across multiple turns:

```python
messages = [SystemMessage("You are a helpful math tutor.")]

while True:
    user_input = input("You: ")
    if user_input.lower() in ["exit", "quit"]:
        break
    
    messages.append(UserMessage(user_input))
    
    response = llm.generate_text_sync(
        LlmRequestParams(model="gpt-4", messages=messages)
    )
    
    assistant_msg = response[0]
    messages.append(assistant_msg)
    
    print(f"Assistant: {assistant_msg.content}")
```

### Pattern 11: Listing Available Models

Query provider for available models:

```python
models = llm.list_models()
print(f"Available models: {models}")
```

## Type Inference and Schema Generation

The SDK automatically converts Python type hints to JSON Schema for tool parameters:

### Supported Types

```python
from typing import Literal, Optional
from datetime import datetime
from enum import Enum

class Unit(Enum):
    CELSIUS = "celsius"
    FAHRENHEIT = "fahrenheit"

def complex_tool(
    # Primitives
    name: str,
    count: int,
    price: float,
    active: bool,
    
    # Optional parameters
    description: Optional[str] = None,
    
    # Enums and Literals
    unit: Unit = Unit.CELSIUS,
    mode: Literal["fast", "slow"] = "fast",
    
    # Collections
    tags: list[str] = [],
    metadata: dict[str, str] = {},
    
    # Datetime types
    created_at: datetime = None,
) -> str:
    """Complex tool with various parameter types."""
    return "Success"
```

**Supported Types:**
- Primitives: `str`, `int`, `float`, `bool`
- Collections: `list`, `dict`, `set`, `tuple`, `Sequence`, `Mapping`
- Special: `datetime`, `date`, `time`, `bytes`
- Type hints: `Optional`, `Union`, `Literal`, `Annotated`
- Custom: `Enum`, `TypedDict`, `dataclass`, Pydantic `BaseModel`

## Error Handling

```python
try:
    response = llm.generate_text_sync(
        LlmRequestParams(
            model="gpt-4",
            messages=[UserMessage("Hello")],
            timeout_sec=5.0
        )
    )
except Exception as e:
    print(f"Error: {e}")
    # Handle timeout, API errors, etc.
```

## Best Practices

1. **Use Type Hints:** Always annotate tool parameters for automatic schema generation
2. **Write Clear Docstrings:** The LLM uses docstrings to understand tool purposes
3. **Set Timeouts:** Prevent hanging requests with `timeout_sec`
4. **Handle Tool Errors:** Wrap tool implementations in try-except for robustness
5. **Stream Long Responses:** Use streaming for better UX with lengthy generations
6. **Maintain Context:** Include conversation history in `messages` for coherent dialogs
7. **Use System Messages:** Set behavior and constraints via `SystemMessage`
8. **Test Tools Independently:** Verify tool functions work before integrating with LLM
9. **Use Async When Possible:** Leverage async/await for I/O-bound operations
10. **Monitor Token Usage:** Track costs via response metadata if needed

## Complete Example: File Analysis Agent

```python
import os
from liteai_sdk import (
    LLM, LlmProviders, LlmRequestParams,
    SystemMessage, UserMessage, AssistantMessage, ToolMessage
)

def read_file(path: str) -> str:
    """Read and return file contents."""
    with open(path, 'r') as f:
        return f.read()

def list_files(directory: str) -> list[str]:
    """List all files in a directory."""
    return os.listdir(directory)

def main():
    llm = LLM(
        provider=LlmProviders.OPENAI,
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url="https://api.openai.com/v1"
    )
    
    messages = [
        SystemMessage("You are a code analysis assistant. Help users understand their files."),
        UserMessage("List files in ./examples and analyze agent_loop.py")
    ]
    
    tools = [read_file, list_files]
    
    # Agent loop
    max_iterations = 5
    for i in range(max_iterations):
        response = llm.generate_text_sync(
            LlmRequestParams(
                model="gpt-4",
                messages=messages,
                tools=tools,
                execute_tools=True
            )
        )
        
        messages.extend(response)
        
        # Check if assistant provided final answer
        has_content = any(
            isinstance(msg, AssistantMessage) and msg.content 
            for msg in response
        )
        
        if has_content and not any(
            isinstance(msg, AssistantMessage) and msg.tool_calls 
            for msg in response
        ):
            # Assistant finished without more tool calls
            break
    
    # Print final response
    for msg in messages:
        if isinstance(msg, AssistantMessage) and msg.content:
            print(f"\nFinal Answer:\n{msg.content}")

if __name__ == "__main__":
    main()
```

## Summary

LiteAI-SDK provides a clean, type-safe interface for building LLM applications with:
- Simple text generation (sync/async)
- Real-time streaming
- Automatic tool calling with type inference
- Agent loop patterns
- Multi-provider support via LiteLLM

The SDK handles the complexity of provider-specific APIs, tool schema generation, and execution flow, letting you focus on building intelligent applications.

---

This guide covers all major functionality of LiteAI-SDK including:
- Basic concepts and initialization
- Text generation patterns (sync/async, streaming/non-streaming)
- Tool calling with three definition methods
- Advanced patterns like agent loops and multi-turn conversations
- Type inference and schema generation
- Best practices and error handling
- Complete working examples

The pattern-based organization (Pattern 1-11) makes it easy for LLMs to quickly locate and understand different usage scenarios.