import asyncio
import os
from dotenv import load_dotenv
from dais_sdk import (
    LLM, LlmProviders, LlmRequestParams,
    UserMessage, AssistantMessage,
    TextChunk, UsageChunk
)

load_dotenv()

llm = LLM(provider=LlmProviders.OPENAI,
          api_key=os.getenv("API_KEY", ""),
          base_url=os.getenv("BASE_URL", ""))

async def main():
    print("=" * 60)
    print("æµå¼è¯·æ±‚ç¤ºä¾‹ - Token ä½¿ç”¨ç»Ÿè®¡")
    print("=" * 60)

    # æµå¼è¯·æ±‚
    message_chunk, full_message_queue = await llm.stream_text(
        LlmRequestParams(
            model="deepseek-v3.1",
            messages=[UserMessage(content="è¯·ç”¨ä¸€å¥è¯ä»‹ç» Python ç¼–ç¨‹è¯­è¨€")]))

    print("\nå®æ—¶å“åº”å†…å®¹ï¼š")
    print("-" * 60)

    # å¤„ç†æµå¼å“åº”å—
    async for chunk in message_chunk:
        match chunk:
            case TextChunk(content=content):
                # å®æ—¶æ‰“å°æ–‡æœ¬å†…å®¹
                print(content, end="", flush=True)
            case UsageChunk(input_tokens=input_t, output_tokens=output_t, total_tokens=total_t):
                # æµå¼å“åº”ç»“æŸæ—¶ä¼šæ”¶åˆ° UsageChunk
                print("\n" + "-" * 60)
                print("\nğŸ“Š Token ä½¿ç”¨ç»Ÿè®¡ (æ¥è‡ªæµå¼å“åº”):")
                print(f"  è¾“å…¥ tokens:   {input_t}")
                print(f"  è¾“å‡º tokens:   {output_t}")
                print(f"  æ€»è®¡ tokens:   {total_t}")

    print("\n" + "=" * 60)

    # è·å–å®Œæ•´æ¶ˆæ¯ï¼ˆåŒ…å« usage ä¿¡æ¯ï¼‰
    full_message = await full_message_queue.get()
    if isinstance(full_message, AssistantMessage) and full_message.usage:
        print("\nğŸ“‹ å®Œæ•´æ¶ˆæ¯çš„ Token ç»Ÿè®¡ï¼š")
        print(f"  è¾“å…¥ tokens (prompt):      {full_message.usage.prompt_tokens}")
        print(f"  è¾“å‡º tokens (completion):  {full_message.usage.completion_tokens}")
        print(f"  æ€»è®¡ tokens:               {full_message.usage.total_tokens}")
        print("=" * 60)

asyncio.run(main())
